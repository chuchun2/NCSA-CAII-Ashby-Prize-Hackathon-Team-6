{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b9d1c8c-5337-4a33-8e68-dd212e36a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a59bf558-aff1-4fe7-91fe-60b6c43d7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.MLP import SimpleMLP, train_step, val_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f70164c8-27c3-46e0-9133-142a2f7e1b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ccn_001', 'ccn_003', 'ccn_006', 'CHI', 'CHI_CCN', 'D_ALPHA',\n",
       "       'D_GAMMA', 'D_ALPHA_CCN', 'D_GAMMA_CCN', 'PM25'], dtype='<U11')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "540d39ea-42ed-475b-8dbd-fa7bc30ef763",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_name = \"ep_13_sunny_D_ALPHA,D_GAMMA,D_ALPHA_CCN,D_GAMMA_CCN_XLONG,XLAT_MLP.pt\"\n",
    "ckpt = torch.load(ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9281d58d-90e5-42df-bdaf-e61256f0d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_datapaths(\n",
    "    train_idx,\n",
    "    val_idx,\n",
    "    files='sunny', \n",
    "    target_cols=[\"ccn_001\", \"ccn_003\", \"ccn_006\"],\n",
    "    exclude_cols = [\"XLONG\", \"XLAT\"],\n",
    "):\n",
    "    # getting colname\n",
    "    featloc = \"/home/kphadke/hackathon/norm_data_timesteps_feat\"\n",
    "    targloc = \"/home/kphadke/hackathon/norm_data_timesteps_targ\"\n",
    "\n",
    "    featcols = np.load(f\"{featloc}/x_name.npy\")\n",
    "    targcols = np.load(f\"{targloc}/y_name.npy\")\n",
    "    \n",
    "    target_idx = [np.where(targcols == i)[0].item() for i in target_cols ]\n",
    "    feat_idx   = [np.where(featcols == i)[0].item() for i in featcols if i not in exclude_cols]\n",
    "    \n",
    "    # getting paths\n",
    "    if files == 'sunny':\n",
    "        # my data\n",
    "        featloc = \"/home/kwoksun2/hackathon_data/norm_data\"\n",
    "        targloc = \"/home/kwoksun2/hackathon_data/norm_data\"\n",
    "\n",
    "        train_feat_files = [f\"{featloc}/t{t}_feat.npy\" for t in train_idx]\n",
    "        val_feat_files   = [f\"{featloc}/t{t}_feat.npy\" for t in val_idx  ]\n",
    "\n",
    "        train_targ_files = [f\"{targloc}/t{t}_targ.npy\" for t in train_idx]\n",
    "        val_targ_files   = [f\"{targloc}/t{t}_targ.npy\" for t in val_idx]\n",
    "    \n",
    "    elif files == 'kedar':\n",
    "        featloc = \"/home/kphadke/hackathon/norm_data_timesteps_feat\"\n",
    "        targloc = \"/home/kphadke/hackathon/norm_data_timesteps_targ\"\n",
    "\n",
    "        train_feat_files = [f\"{featloc}/{t}feat_norm.npy\" for t in train_idx]\n",
    "        val_feat_files   = [f\"{featloc}/{t}feat_norm.npy\" for t in val_idx  ]\n",
    "\n",
    "        train_targ_files = [f\"{targloc}/{t}targ_norm.npy\" for t in train_idx]\n",
    "        val_targ_files   = [f\"{targloc}/{t}targ_norm.npy\" for t in val_idx]\n",
    "\n",
    "    return [np.array(train_feat_files),np.array(train_targ_files)], [np.array(val_feat_files),np.array(val_targ_files)], feat_idx, target_idx\n",
    "\n",
    "\n",
    "\n",
    "model = ckpt['model_state_dict']\n",
    "datasource = ckpt['datasource']\n",
    "exclude_cols =  ckpt['exclude_cols']\n",
    "target_cols =  ckpt['target_cols']\n",
    "\n",
    "# getting the idx used in the model\n",
    "featloc = \"/home/kphadke/hackathon/norm_data_timesteps_feat\"\n",
    "targloc = \"/home/kphadke/hackathon/norm_data_timesteps_targ\"\n",
    "\n",
    "featcols = np.load(f\"{featloc}/x_name.npy\")\n",
    "targcols = np.load(f\"{targloc}/y_name.npy\")\n",
    "\n",
    "target_idx = [np.where(targcols == i)[0].item() for i in target_cols ]\n",
    "feat_idx   = [np.where(featcols == i)[0].item() for i in featcols if i not in exclude_cols]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# THIS IS JUST FOR TESTING!!!!!!!\n",
    "# WILL CHANGE BACK TO 133 later\n",
    "nsnapshots = 133\n",
    "\n",
    "# DONT CHANGE THE SEEEDDDDDD!!!!!\n",
    "np.random.seed(52)\n",
    "full_indx = np.arange(nsnapshots)\n",
    "train_idx, val_idx = train_test_split(full_indx, test_size=0.2, train_size=0.8) \n",
    "\n",
    "train_files, val_files, feat_idx, target_idx = getting_datapaths(\n",
    "    train_idx, \n",
    "    val_idx, \n",
    "    files=datasource, \n",
    "    exclude_cols = exclude_cols,\n",
    "    target_cols  = target_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adf5de37-3857-4bac-8b51-9d82b2c24a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kedar\n",
    "if datasource == 'kedar':\n",
    "    test_feats = [f\"/home/kphadke/hackathon/norm_data_timesteps_feat_eval/{i}feat_norm_eval.npy\" for i in range(12)]\n",
    "    test_targs = [f\"/home/kphadke/hackathon/norm_data_timesteps_targ_eval/{i}targ_norm_eval.npy\" for i in range(12)]\n",
    "else:\n",
    "    test_feats = [f\"/home/kwoksun2/hackathon_data/norm_data_val/t{i}_feat.npy\" for i in range(12)]\n",
    "    test_targs = [f\"/home/kwoksun2/hackathon_data/norm_data_val/t{i}_targ.npy\" for i in range(12)]\n",
    "#     raise ValueError\n",
    "    \n",
    "if datasource == \"sunny\":\n",
    "    target_std= np.array(\n",
    "        [1.3428156,\n",
    "        1.0779657,\n",
    "        1.0479535,\n",
    "        0.08097392,\n",
    "        0.10943509 ,\n",
    "        0.20763765 ,\n",
    "        0.2632665 ,\n",
    "        0.07147439,\n",
    "        0.091097355,\n",
    "        1.1029556\n",
    "        ])\n",
    "    \n",
    "    tstd = target_std[target_idx]\n",
    "    \n",
    "elif datasource == \"kedar\":\n",
    "    target_std = np.load(\"/home/kphadke/hackathon/targ_std.npy\").transpose()\n",
    "    tstd = target_std[:,target_idx]\n",
    "    tstd = tstd[:,np.newaxis,np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fad9d87-d7e1-455e-9925-f2699d4e2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [test_feats, test_targs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d1ac5a3-d4cc-408e-a2a7-bc7a96a1f9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ccn_001', 'ccn_003', 'ccn_006', 'CHI', 'CHI_CCN', 'D_ALPHA',\n",
       "       'D_GAMMA', 'D_ALPHA_CCN', 'D_GAMMA_CCN', 'PM25'], dtype='<U11')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fa9d3be-85c3-47c6-b007-224c8f98291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_name = f'predictions_{ckpt_name.replace(\".pt\",\"\")}'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c0dbb49-c527-4a1d-b995-935c6b86c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prediction dump\n",
    "feat_files, targ_files = test_files\n",
    "# we are selecting only the first validation data\n",
    "for i in range(len(feat_files)):\n",
    "    feat_data = np.load(feat_files[i])\n",
    "    targ_data = np.load(targ_files[i])\n",
    "\n",
    "    if targ_data.ndim == 1:\n",
    "        targ_data = targ_data.reshape(10,-1).transpose()\n",
    "\n",
    "    tds = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(feat_data[:,feat_idx]).float(), \n",
    "        torch.from_numpy(targ_data[:,target_idx]).float()\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(tds, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # turn it in to a data loader for accessing\n",
    "    ypred = []\n",
    "    ytruth = []\n",
    "    xfeats = []\n",
    "    with torch.no_grad():\n",
    "        for feat, targ in dl:\n",
    "            ypred.append(model(feat.cuda()).cpu().numpy()[:,:4])\n",
    "            ytruth.append(targ.numpy()[:,:4])\n",
    "            xfeats.append(feat.numpy())\n",
    "\n",
    "    ypred = np.vstack(ypred)\n",
    "    ytruth = np.vstack(ytruth)\n",
    "    xfeats = np.vstack(xfeats)\n",
    "    \n",
    "    np.save(f\"{folder_name}/{i}_xfeats.npy\", xfeats)\n",
    "    np.save(f\"{folder_name}/{i}_ypred.npy\", ypred)\n",
    "    np.save(f\"{folder_name}/{i}_ytruth.npy\", ytruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8a2ef-2ad2-46ee-b847-500c281fa6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
